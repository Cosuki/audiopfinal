{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-tASMJUfdmR"
   },
   "source": [
    "# Proyecto final de Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sl5LWceGfdmT"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "fpath = '/content/audiopfinal/data'\n",
    "\n",
    "\n",
    "token = userdata.get('ghToken')\n",
    "!git clone https://{token}@github.com/Cosuki/audiopfinal.git > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io, signal\n",
    "from scipy.io.wavfile import read\n",
    "from IPython.display import Audio\n",
    "import scipy.io.wavfile as wav\n",
    "from matplotlib.pyplot import specgram\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "fpath = './data'\n",
    "\n",
    "def pltlegacy(ax, title, xLabel, yLabel):\n",
    "  if type(ax) == np.ndarray or type(ax) == plt.Axes:\n",
    "    ax.set_title(title)\n",
    "    # ax.set_ylim(top=10000)\n",
    "    ax.set_xlabel(xLabel)\n",
    "    ax.set_ylabel(yLabel)\n",
    "    if not yLabel: ax.get_yaxis().set_visible(False)\n",
    "    # if not xLabel: ax.get_xaxis().set_visible(False)\n",
    "  else:\n",
    "    ax.title(title)\n",
    "    # ax.set_ylim(top=10000)\n",
    "    ax.xlabel(xLabel)\n",
    "    ax.ylabel(yLabel)\n",
    "\n",
    "def getpltmtplt(ax, key, data, sr, xLabel, yLabel):\n",
    "    Pxx, freqs, bins, im = ax.specgram(data, NFFT=1024, srG=sr, noverlap=128, cmap='viridis')\n",
    "    pltlegacy(ax, f'{key}', xLabel, yLabel)\n",
    "    return Pxx, freqs, bins, im\n",
    "\n",
    "def getSpecSig(ax, key, data, sr, xLabel, yLabel):\n",
    "  f, t_spec, Sxx = spectrogram(data, sr)\n",
    "  Sxx += 1e-10\n",
    "  pltlegacy(ax, f'{key}', xLabel, yLabel)\n",
    "  return ax.pcolormesh(t_spec, f, 10 * np.log10(Sxx), shading='gouraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "J1NSNtuwgJyE",
    "outputId": "ac523b80-de7e-46f1-f075-724e2ec7654f"
   },
   "outputs": [],
   "source": [
    "srG, guitar = io.wavfile.read(f'{fpath}/Police-vocals-guitar.wav')\n",
    "srV, voice = io.wavfile.read(f'{fpath}/Emily_Linge-vocals.wav')\n",
    "\n",
    "print(f'{srG=}, {srV=}')\n",
    "\n",
    "guitar = guitar[0: 30*srG]\n",
    "voice = voice[0: 30*srV]\n",
    "\n",
    "display(Audio(guitar, rate=srG), Audio(voice, rate=srV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import toeplitz, solve\n",
    "import numpy as np\n",
    "\n",
    "def get_lpc(s, p=20):\n",
    "    \"\"\"Compute the LPC analysis using the autocorrelation method.\"\"\"\n",
    "    N = s.shape[0]\n",
    "    p = min(p, N - 1)  # Asegurar que p no sea mayor que el tamaño del marco\n",
    "\n",
    "    # Compute autocorrelation values\n",
    "    r = np.zeros((p + 1,))\n",
    "    for k in range(p + 1):\n",
    "        r[k] = np.dot(s[:N - k], s[k:])\n",
    "\n",
    "    if np.all(r == 0):\n",
    "        raise ValueError(\"Autocorrelation is zero; the input signal may be silent or constant.\")\n",
    "\n",
    "    # Solve to compute model coefficients\n",
    "    try:\n",
    "        ak = solve(toeplitz(r[:p]) + 1e-6 * np.eye(p), r[1:p + 1]).squeeze()\n",
    "    except np.linalg.LinAlgError:\n",
    "        raise ValueError(\"Matrix is singular. Adjust the model order or check input data.\")\n",
    "\n",
    "    # Compute mean squared error\n",
    "    e = r[0] - np.dot(ak.T, r[1:p + 1])\n",
    "\n",
    "    # Compute normalized mean squared error\n",
    "    e_norm = e / r[0] if r[0] != 0 else 0\n",
    "\n",
    "    return ak, e, e_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "from numpy import dtype\n",
    "from numpy._typing._array_like import NDArray\n",
    "\n",
    "\n",
    "def lpc_analysis(sig, sr, t, getPlot=False, p=0, retType='dB', Ndft=1024):\n",
    "    n = int(t * sr)\n",
    "    window = signal.windows.get_window('hann', n)  # Smoothing window\n",
    "    lpc_matrix = []\n",
    "    threshold = 1e-12  # Umbral para detectar silencio\n",
    "    \n",
    "    i = 0\n",
    "    for chunk in np.array_split(sig, int(len(sig) / n)):\n",
    "        i += 1\n",
    "        if len(chunk) < n:\n",
    "            chunk = np.pad(chunk, (0, n - len(chunk)), mode='constant')  # Padding\n",
    "        \n",
    "        window = signal.windows.get_window('hann', len(chunk))  # Ajustar ventana\n",
    "        s_win = chunk * window  # Señal con ventana\n",
    "        \n",
    "        # Detectar silencio\n",
    "        if np.max(np.abs(s_win)) < threshold:\n",
    "            # print(f\"Skipping silent segment {i}\")\n",
    "            lpc_matrix.append(np.zeros(Ndft // 2))\n",
    "            continue\n",
    "        \n",
    "        # Normalizar segmento\n",
    "        s_win = s_win / (np.max(np.abs(s_win)) + 1e-12)\n",
    "        \n",
    "        try:\n",
    "            ak, e, e_norm = get_lpc(s_win, p if p else int(sr / 1000))\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error processing segment: {ve}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # Espectro de magnitud\n",
    "        X = np.fft.fft(s_win, Ndft)\n",
    "        magX = np.abs(X[:Ndft // 2])\n",
    "        \n",
    "        # filter obtained from the lpc analysis\n",
    "        S = 1\n",
    "        U = np.concatenate([[1], -ak])\n",
    "        G = np.sqrt(e) # compute gain\n",
    "        w, H = signal.freqz(G*S, U, worN=Ndft, whole=True) # compute the frequency response of the digital filter\n",
    "        magH = np.abs(H[:Ndft // 2])\n",
    "        \n",
    "        lpc_matrix.append(20 * np.log10(magH) if retType == 'dB' else magX)\n",
    "        \n",
    "        if getPlot:\n",
    "            plt.plot(20 * np.log10(magX))\n",
    "            plt.title(\"Magnitude Spectrum\")\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "    return np.array(lpc_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = .02\n",
    "\n",
    "a = lpc_analysis(guitar, srG, t)\n",
    "b = lpc_analysis(voice, srV, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(stop=100, step=4)\n",
    "print(a.shape, b.shape)\n",
    "for i in np.arange(stop=3000, step=5):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(25, 4))\n",
    "    axs[0].plot(a[i])\n",
    "    axs[0].plot(b[i])\n",
    "    axs[1].plot(a[i+1])\n",
    "    axs[1].plot(b[i+1])\n",
    "    axs[2].plot(a[i+2])\n",
    "    axs[2].plot(b[i+2])\n",
    "    axs[3].plot(a[i+3])\n",
    "    axs[3].plot(b[i+3])\n",
    "    axs[4].plot(a[i+4])\n",
    "    axs[4].plot(b[i+4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que estas son las matrices LPC de las dos señales\n",
    "# lpc_matrix1: Matriz de LPC de la primera señal (N1 x D)\n",
    "# lpc_matrix2: Matriz de LPC de la segunda señal (N2 x D)\n",
    "\n",
    "# Calcula la alineación DTW entre las dos matrices\n",
    "distance, path = fastdtw(a, b, radius=300, dist=euclidean)\n",
    "# Extraer los índices de alineación\n",
    "indices1, indices2 = zip(*path)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(indices1, indices2, 'o-', label='Warping Path', markersize=.1)\n",
    "plt.xlabel('Tramos de guitar')\n",
    "plt.ylabel('Tramos de voice')\n",
    "plt.title('Alineación de DTW')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_signal2 = []\n",
    "\n",
    "voice_splited = np.array_split(voice, int(len(voice) / int(t * srG)))\n",
    "for idx1, idx2 in path:\n",
    "    aligned_signal2.extend(voice_splited[idx2])  # Toma el tramo correspondiente\n",
    "\n",
    "aligned_signal2 = np.array(aligned_signal2)\n",
    "\n",
    "print(aligned_signal2.shape)\n",
    "index = min(len(aligned_signal2), len(guitar))\n",
    "suma = []\n",
    "suma.append(guitar[:index])\n",
    "suma.append(aligned_signal2[:index])\n",
    "display(Audio(suma, rate=srV))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastdtw import fastdtw\n",
    "from scipy.signal import resample\n",
    "\n",
    "def synchronize_tracks(lpc1, lpc2, signal1, signal2, sr):\n",
    "    \"\"\"\n",
    "    Sincroniza dos pistas de audio usando DTW y matrices LPC.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lpc1 : numpy.ndarray\n",
    "        LPC matrix de la primera señal.\n",
    "    lpc2 : numpy.ndarray\n",
    "        LPC matrix de la segunda señal.\n",
    "    signal1 : numpy.ndarray\n",
    "        Señal original de la primera pista.\n",
    "    signal2 : numpy.ndarray\n",
    "        Señal original de la segunda pista.\n",
    "    sr : int\n",
    "        Frecuencia de muestreo de las señales.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adjusted_signal1 : numpy.ndarray\n",
    "        Primera señal ajustada a la alineación de la segunda.\n",
    "    \"\"\"\n",
    "    # Calcular DTW entre las matrices LPC\n",
    "    dist, path = fastdtw(lpc1, lpc2, dist=euclidean)\n",
    "    print(f\"DTW alignment distance: {dist} {np.array(path).shape}\")\n",
    "    \n",
    "    # Mapeo de alineación\n",
    "    alignment_1_to_2 = path[0]\n",
    "    alignment_2_to_1 = path[1]\n",
    "    \n",
    "    # Ajustar la primera señal para alinearse con la segunda\n",
    "    adjusted_signal1 = []\n",
    "    splited_signal2 = np.array_split(signal2, int(len(signal2) / int(.030 * srG)))\n",
    "    for idx1, idx2 in zip(alignment_1_to_2, alignment_2_to_1):\n",
    "        segment = splited_signal2[idx2]\n",
    "        if len(segment) > 0:\n",
    "            target_length = int(sr * 0.03)\n",
    "            adjusted_segment = resample(segment, target_length)\n",
    "            adjusted_signal1.append(adjusted_segment)\n",
    "    \n",
    "    print(np.array(adjusted_signal1).shape)\n",
    "    \n",
    "    # Convertir a una señal única\n",
    "    adjusted_signal1 = np.concatenate(adjusted_signal1)\n",
    "    \n",
    "    return adjusted_signal1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumamos que tienes las matrices LPC y señales originales\n",
    "adjusted_track = synchronize_tracks(a, b, guitar, voice, srG)\n",
    "\n",
    "\n",
    "\n",
    "print(adjusted_track.shape)\n",
    "Audio(adjusted_track, rate=srV)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
